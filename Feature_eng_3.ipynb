{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e4cd45",
   "metadata": {},
   "source": [
    "**Q1. Min-Max Scaling:**\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale and transform the values of numerical features to a specific range. The purpose is to bring all the features to a common scale without distorting the differences in the ranges of the original data. The typical range used is [0, 1], but it can be adjusted to any desired range.\n",
    "\n",
    "**Usage in Data Preprocessing:**\n",
    "\n",
    "1. **Equalizing Scales:** Min-Max scaling is used to ensure that all features are on a similar scale. This is important when working with algorithms that are sensitive to the scale of input features, such as gradient descent-based optimization algorithms.\n",
    "\n",
    "2. **Distance-Based Algorithms:** It is particularly crucial for distance-based algorithms (e.g., k-nearest neighbors, clustering) where the calculation of distances between data points is influenced by the scale of the features. Scaling prevents features with larger ranges from dominating the distance calculations.\n",
    "\n",
    "3. **Neural Networks:** When training neural networks, normalization can accelerate convergence by ensuring that the weights are updated more uniformly across features.\n",
    "\n",
    "4. **Visualization:** Min-Max scaling is useful when plotting or visualizing data, as it prevents certain features from disproportionately influencing the visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98475937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_bill   tip     sex smoker  day    time  size\n",
      "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
      "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
      "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
      "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
      "4       24.59  3.61  Female     No  Sun  Dinner     4\n",
      "[[0.29157939 0.00111111 0.2       ]\n",
      " [0.1522832  0.07333333 0.4       ]\n",
      " [0.3757855  0.27777778 0.4       ]\n",
      " [0.43171345 0.25666667 0.2       ]\n",
      " [0.45077503 0.29       0.6       ]\n",
      " [0.46543779 0.41222222 0.6       ]\n",
      " [0.11939673 0.11111111 0.2       ]\n",
      " [0.49874319 0.23555556 0.6       ]\n",
      " [0.25073314 0.10666667 0.2       ]\n",
      " [0.24528697 0.24777778 0.2       ]\n",
      " [0.15081693 0.07888889 0.2       ]\n",
      " [0.67427734 0.44444444 0.6       ]\n",
      " [0.25869292 0.06333333 0.2       ]\n",
      " [0.32174277 0.22222222 0.6       ]\n",
      " [0.24633431 0.22444444 0.2       ]\n",
      " [0.38772518 0.32444444 0.2       ]\n",
      " [0.15207373 0.07444444 0.4       ]\n",
      " [0.27691663 0.30111111 0.4       ]\n",
      " [0.29116045 0.27777778 0.4       ]\n",
      " [0.36824466 0.26111111 0.4       ]\n",
      " [0.31105991 0.34222222 0.2       ]\n",
      " [0.36070381 0.19444444 0.2       ]\n",
      " [0.2660243  0.13666667 0.2       ]\n",
      " [0.761416   0.73111111 0.6       ]\n",
      " [0.35085882 0.24222222 0.2       ]\n",
      " [0.30875576 0.14888889 0.6       ]\n",
      " [0.21575199 0.11111111 0.2       ]\n",
      " [0.20150817 0.11111111 0.2       ]\n",
      " [0.39023879 0.36666667 0.2       ]\n",
      " [0.34729786 0.22222222 0.2       ]\n",
      " [0.13573523 0.05       0.2       ]\n",
      " [0.32006703 0.16666667 0.6       ]\n",
      " [0.25115207 0.22222222 0.2       ]\n",
      " [0.36908253 0.16111111 0.6       ]\n",
      " [0.30812736 0.25222222 0.2       ]\n",
      " [0.43967323 0.28888889 0.4       ]\n",
      " [0.27733557 0.11111111 0.4       ]\n",
      " [0.29032258 0.23       0.4       ]\n",
      " [0.32718894 0.14555556 0.4       ]\n",
      " [0.59069962 0.44444444 0.4       ]\n",
      " [0.27167993 0.13777778 0.4       ]\n",
      " [0.30142438 0.17111111 0.2       ]\n",
      " [0.22769166 0.22888889 0.2       ]\n",
      " [0.13845832 0.03555556 0.2       ]\n",
      " [0.57247591 0.51111111 0.6       ]\n",
      " [0.31881022 0.22222222 0.2       ]\n",
      " [0.40134059 0.44444444 0.2       ]\n",
      " [0.6143695  0.55555556 0.6       ]\n",
      " [0.53372434 0.11666667 0.4       ]\n",
      " [0.31357352 0.22222222 0.2       ]\n",
      " [0.19836615 0.16666667 0.2       ]\n",
      " [0.15123586 0.17777778 0.2       ]\n",
      " [0.66485128 0.46666667 0.6       ]\n",
      " [0.14390448 0.06222222 0.2       ]\n",
      " [0.47109342 0.37111111 0.6       ]\n",
      " [0.34394638 0.27888889 0.2       ]\n",
      " [0.73188102 0.22222222 0.6       ]\n",
      " [0.4888982  0.05555556 0.2       ]\n",
      " [0.17113532 0.08444444 0.2       ]\n",
      " [0.94679514 0.63666667 0.6       ]\n",
      " [0.36070381 0.24555556 0.2       ]\n",
      " [0.22496858 0.11111111 0.2       ]\n",
      " [0.16652702 0.10888889 0.2       ]\n",
      " [0.31881022 0.30666667 0.6       ]\n",
      " [0.30414747 0.18222222 0.4       ]\n",
      " [0.35630499 0.23888889 0.4       ]\n",
      " [0.28026812 0.16333333 0.2       ]\n",
      " [0.         0.         0.        ]\n",
      " [0.359447   0.11222222 0.2       ]\n",
      " [0.25010473 0.12111111 0.2       ]\n",
      " [0.18747382 0.10777778 0.2       ]\n",
      " [0.29325513 0.22222222 0.4       ]\n",
      " [0.49832426 0.23777778 0.2       ]\n",
      " [0.46522832 0.44444444 0.2       ]\n",
      " [0.24423963 0.13333333 0.2       ]\n",
      " [0.15584416 0.02777778 0.2       ]\n",
      " [0.31105991 0.23111111 0.2       ]\n",
      " [0.50544617 0.33333333 0.6       ]\n",
      " [0.4124424  0.22222222 0.2       ]\n",
      " [0.29786343 0.19       0.2       ]\n",
      " [0.34289904 0.22222222 0.2       ]\n",
      " [0.28466695 0.26666667 0.2       ]\n",
      " [0.14662757 0.09222222 0.        ]\n",
      " [0.6202346  0.44444444 0.2       ]\n",
      " [0.27042313 0.11444444 0.2       ]\n",
      " [0.66527021 0.46333333 0.6       ]\n",
      " [0.20863008 0.11111111 0.2       ]\n",
      " [0.31860075 0.33333333 0.2       ]\n",
      " [0.45328865 0.53888889 0.2       ]\n",
      " [0.37892752 0.22222222 0.2       ]\n",
      " [0.54252199 0.22222222 0.2       ]\n",
      " [0.40678676 0.27777778 0.2       ]\n",
      " [0.05613741 0.         0.2       ]\n",
      " [0.27754504 0.36666667 0.2       ]\n",
      " [0.41223293 0.25       0.2       ]\n",
      " [0.7771261  0.41444444 0.6       ]\n",
      " [0.50712191 0.33333333 0.2       ]\n",
      " [0.18768328 0.05555556 0.2       ]\n",
      " [0.3757855  0.22222222 0.2       ]\n",
      " [0.19669041 0.05555556 0.2       ]\n",
      " [0.17343946 0.16666667 0.2       ]\n",
      " [0.25785505 0.22222222 0.2       ]\n",
      " [0.86363636 0.16666667 0.4       ]\n",
      " [0.40532049 0.27555556 0.2       ]\n",
      " [0.37390029 0.34222222 0.2       ]\n",
      " [0.25743611 0.07111111 0.2       ]\n",
      " [0.36489317 0.34       0.2       ]\n",
      " [0.46376204 0.36555556 0.2       ]\n",
      " [0.31776288 0.30666667 0.2       ]\n",
      " [0.23544198 0.33333333 0.2       ]\n",
      " [0.22894847 0.22222222 0.2       ]\n",
      " [0.0875576  0.         0.        ]\n",
      " [0.73313783 0.33333333 0.4       ]\n",
      " [0.43736908 0.17222222 0.2       ]\n",
      " [0.47423544 0.33333333 0.4       ]\n",
      " [0.29828236 0.27777778 0.2       ]\n",
      " [0.56263092 0.45222222 0.6       ]\n",
      " [0.15877671 0.05555556 0.2       ]\n",
      " [0.196062   0.08888889 0.2       ]\n",
      " [0.44009217 0.21333333 0.6       ]\n",
      " [0.18056137 0.14555556 0.2       ]\n",
      " [0.21679933 0.07555556 0.2       ]\n",
      " [0.23439464 0.16666667 0.2       ]\n",
      " [0.26979472 0.11111111 0.2       ]\n",
      " [0.19710934 0.16888889 0.2       ]\n",
      " [0.55990783 0.35555556 1.        ]\n",
      " [0.11416003 0.05333333 0.2       ]\n",
      " [0.2398408  0.11111111 0.2       ]\n",
      " [0.17406787 0.11111111 0.2       ]\n",
      " [0.4136992  0.13111111 0.4       ]\n",
      " [0.33535819 0.05555556 0.2       ]\n",
      " [0.36028488 0.20333333 0.2       ]\n",
      " [0.16966904 0.05555556 0.2       ]\n",
      " [0.19250105 0.11111111 0.2       ]\n",
      " [0.31818182 0.25       0.2       ]\n",
      " [0.11395057 0.02777778 0.2       ]\n",
      " [0.15207373 0.11111111 0.2       ]\n",
      " [0.23209049 0.11111111 0.2       ]\n",
      " [0.27084206 0.11111111 0.2       ]\n",
      " [0.21135316 0.19444444 0.2       ]\n",
      " [0.30163385 0.27777778 0.2       ]\n",
      " [0.65416841 0.63333333 1.        ]\n",
      " [0.79849183 0.44444444 0.8       ]\n",
      " [0.50230415 0.44444444 1.        ]\n",
      " [0.27984918 0.14444444 0.2       ]\n",
      " [0.11059908 0.05555556 0.2       ]\n",
      " [0.3261416  0.04       0.4       ]\n",
      " [0.1843318  0.07       0.2       ]\n",
      " [0.140553   0.08111111 0.2       ]\n",
      " [0.09300377 0.11111111 0.2       ]\n",
      " [0.23041475 0.16666667 0.2       ]\n",
      " [0.21072476 0.11111111 0.2       ]\n",
      " [0.29723502 0.19333333 0.4       ]\n",
      " [0.44993716 0.11111111 0.6       ]\n",
      " [0.34981148 0.11111111 0.6       ]\n",
      " [0.56095517 0.46       0.8       ]\n",
      " [0.94470046 0.44444444 1.        ]\n",
      " [0.45936322 0.30555556 0.6       ]\n",
      " [0.21617093 0.17888889 0.2       ]\n",
      " [0.28110599 0.11111111 0.6       ]\n",
      " [0.38604943 0.27777778 0.6       ]\n",
      " [0.20087977 0.16666667 0.2       ]\n",
      " [0.27524089 0.11111111 0.4       ]\n",
      " [0.22496858 0.11111111 0.2       ]\n",
      " [0.30247172 0.22222222 0.2       ]\n",
      " [0.44930876 0.27555556 0.4       ]\n",
      " [0.37054881 0.13777778 0.2       ]\n",
      " [0.59991621 0.38888889 0.6       ]\n",
      " [0.1575199  0.06777778 0.2       ]\n",
      " [0.15835777 0.11111111 0.2       ]\n",
      " [1.         1.         0.4       ]\n",
      " [0.26686217 0.24       0.2       ]\n",
      " [0.0875576  0.46111111 0.2       ]\n",
      " [0.60284876 0.24222222 0.2       ]\n",
      " [0.28801843 0.33333333 0.2       ]\n",
      " [0.6248429  0.23444444 0.2       ]\n",
      " [0.3104315  0.11111111 0.2       ]\n",
      " [0.23900293 0.11111111 0.2       ]\n",
      " [0.13678257 0.33333333 0.2       ]\n",
      " [0.66108085 0.28333333 0.2       ]\n",
      " [0.66149979 0.29777778 0.6       ]\n",
      " [0.42438207 0.51666667 0.2       ]\n",
      " [0.8856305  0.27777778 0.4       ]\n",
      " [0.42103058 0.61111111 0.6       ]\n",
      " [0.78508588 0.22222222 0.2       ]\n",
      " [0.36908253 0.44444444 0.8       ]\n",
      " [0.37348136 0.27777778 0.4       ]\n",
      " [0.57373272 0.11111111 0.8       ]\n",
      " [0.31587767 0.27777778 0.4       ]\n",
      " [0.41956431 0.33333333 0.4       ]\n",
      " [0.26434855 0.05555556 0.2       ]\n",
      " [0.35064935 0.35444444 0.2       ]\n",
      " [0.53142019 0.17333333 0.2       ]\n",
      " [0.25994973 0.11333333 0.2       ]\n",
      " [0.2829912  0.33333333 0.2       ]\n",
      " [0.09405111 0.04888889 0.2       ]\n",
      " [0.1522832  0.11111111 0.2       ]\n",
      " [0.83870968 0.44444444 0.6       ]\n",
      " [0.20800168 0.11111111 0.2       ]\n",
      " [0.21868454 0.11111111 0.2       ]\n",
      " [0.32760788 0.33333333 0.4       ]\n",
      " [0.20255551 0.11222222 0.2       ]\n",
      " [0.20800168 0.11111111 0.2       ]\n",
      " [0.27922078 0.16666667 0.2       ]\n",
      " [0.36573104 0.33333333 0.6       ]\n",
      " [0.28068705 0.24777778 0.4       ]\n",
      " [0.49266862 0.26777778 0.4       ]\n",
      " [0.74696271 0.22222222 0.6       ]\n",
      " [0.44407206 0.11444444 0.2       ]\n",
      " [0.20297444 0.13666667 0.2       ]\n",
      " [0.565354   0.11111111 0.4       ]\n",
      " [0.47800587 0.46222222 0.6       ]\n",
      " [0.94805195 0.88888889 0.6       ]\n",
      " [0.21365731 0.16666667 0.2       ]\n",
      " [0.52576456 0.61111111 0.4       ]\n",
      " [0.205907   0.01111111 0.2       ]\n",
      " [0.52534562 0.22222222 0.8       ]\n",
      " [0.17846669 0.05555556 0.2       ]\n",
      " [0.09782153 0.04888889 0.2       ]\n",
      " [0.56702974 0.23222222 0.6       ]\n",
      " [0.19040637 0.13333333 0.2       ]\n",
      " [0.21679933 0.27555556 0.2       ]\n",
      " [0.11541684 0.10222222 0.        ]\n",
      " [0.27042313 0.22222222 0.4       ]\n",
      " [0.21679933 0.06444444 0.2       ]\n",
      " [0.2764977  0.16666667 0.2       ]\n",
      " [0.1470465  0.11111111 0.2       ]\n",
      " [0.3640553  0.22222222 0.6       ]\n",
      " [0.21386678 0.19111111 0.2       ]\n",
      " [0.39903645 0.20888889 0.2       ]\n",
      " [0.43862589 0.11111111 0.6       ]\n",
      " [0.26434855 0.22222222 0.4       ]\n",
      " [0.17888563 0.26555556 0.2       ]\n",
      " [0.16129032 0.05222222 0.2       ]\n",
      " [0.26099707 0.22222222 0.2       ]\n",
      " [0.14662757 0.02777778 0.2       ]\n",
      " [0.19962296 0.         0.2       ]\n",
      " [0.62337662 0.01888889 0.2       ]\n",
      " [0.68621701 0.40777778 0.4       ]\n",
      " [0.5437788  0.54666667 0.4       ]\n",
      " [0.50502723 0.11111111 0.2       ]\n",
      " [0.41055718 0.11111111 0.2       ]\n",
      " [0.30896523 0.08333333 0.2       ]\n",
      " [0.32907415 0.22222222 0.2       ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "df = sns.load_dataset('tips')\n",
    "print(df.head())\n",
    "m_M_scaler = MinMaxScaler()\n",
    "x_scaled = m_M_scaler.fit_transform(df[['total_bill', 'tip', 'size']])\n",
    "print(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05aee6a",
   "metadata": {},
   "source": [
    "**Q2. Unit Vector Technique in Feature Scaling:**\n",
    "\n",
    "The unit vector technique, also known as vector normalization or unit normalization, is a feature scaling method that scales each data point in a feature vector by dividing it by the Euclidean norm (magnitude) of the entire vector. This normalization technique ensures that the resulting vector has a magnitude of 1, effectively converting it into a unit vector. The unit vector technique is primarily used for situations where the direction of the vector is more important than its magnitude.\n",
    "\n",
    "*Difference from Min-Max Scaling:*\n",
    "\n",
    "- Min-Max scaling scales each feature independently and transforms the values to a specific range (e.g., [0, 1]).\n",
    "- Unit vector scaling scales the entire vector, ensuring that its magnitude becomes 1, preserving the direction of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "698b3e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99823771 0.05934197]\n",
      " [0.98735707 0.15851187]\n",
      " [0.98640661 0.16432285]\n",
      " [0.99037159 0.13843454]\n",
      " [0.98939488 0.14525073]\n",
      " [0.98309589 0.18309141]\n",
      " [0.97496878 0.2223418 ]\n",
      " [0.99333102 0.11529735]\n",
      " [0.99161511 0.12922644]\n",
      " [0.97694312 0.21349975]\n",
      " [0.98641987 0.16424323]\n",
      " [0.99009498 0.14039917]\n",
      " [0.99485672 0.10129216]\n",
      " [0.98700924 0.16066347]\n",
      " [0.97988851 0.19954574]\n",
      " [0.98389908 0.17872495]\n",
      " [0.9871829  0.15959298]\n",
      " [0.9750328  0.22206088]\n",
      " [0.97938658 0.20199487]\n",
      " [0.98709527 0.1601341 ]\n",
      " [0.97504726 0.22199737]\n",
      " [0.9909398  0.13430677]\n",
      " [0.99014941 0.14001479]\n",
      " [0.98201    0.18882891]\n",
      " [0.98737215 0.15841793]\n",
      " [0.99147891 0.1302673 ]\n",
      " [0.98899596 0.14794255]\n",
      " [0.98780711 0.15568276]\n",
      " [0.98092686 0.19437721]\n",
      " [0.98854552 0.15092298]\n",
      " [0.98866899 0.15011205]\n",
      " [0.99084659 0.13499272]\n",
      " [0.98073067 0.19536467]\n",
      " [0.99306186 0.11759312]\n",
      " [0.98350502 0.18088084]\n",
      " [0.98899056 0.14797864]\n",
      " [0.9925654  0.1217125 ]\n",
      " [0.98395349 0.17842512]\n",
      " [0.99244848 0.12266217]\n",
      " [0.98745639 0.15789197]\n",
      " [0.99038917 0.13830871]\n",
      " [0.9895835  0.14396003]\n",
      " [0.97674434 0.21440729]\n",
      " [0.99083017 0.1351132 ]\n",
      " [0.98345319 0.18116243]\n",
      " [0.98681354 0.16186116]\n",
      " [0.9756262  0.21943909]\n",
      " [0.983282   0.18208926]\n",
      " [0.99743203 0.07161946]\n",
      " [0.98645298 0.16404429]\n",
      " [0.98070081 0.19551452]\n",
      " [0.96952977 0.24497351]\n",
      " [0.98902579 0.14774301]\n",
      " [0.98790759 0.15504385]\n",
      " [0.98588897 0.16740055]\n",
      " [0.98416747 0.17724104]\n",
      " [0.99689977 0.07868191]\n",
      " [0.99839096 0.05670528]\n",
      " [0.98796171 0.15469863]\n",
      " [0.99041991 0.13808838]\n",
      " [0.98771556 0.15626254]\n",
      " [0.98967534 0.14332735]\n",
      " [0.98423933 0.17684155]\n",
      " [0.97951611 0.20136581]\n",
      " [0.98892398 0.14842293]\n",
      " [0.98791805 0.15497718]\n",
      " [0.98891429 0.14848744]\n",
      " [0.95082902 0.30971629]\n",
      " [0.9951003  0.09887057]\n",
      " [0.99044476 0.13791003]\n",
      " [0.98683411 0.16173571]\n",
      " [0.9849053  0.17309408]\n",
      " [0.99323616 0.11611175]\n",
      " [0.98099635 0.19402618]\n",
      " [0.98902973 0.14771659]\n",
      " [0.99300147 0.11810198]\n",
      " [0.98554889 0.16939122]\n",
      " [0.98935914 0.14549399]\n",
      " [0.99142462 0.13067987]\n",
      " [0.98793839 0.15484749]\n",
      " [0.98830106 0.1525156 ]\n",
      " [0.97980406 0.19996001]\n",
      " [0.98388554 0.17879946]\n",
      " [0.98849725 0.15123887]\n",
      " [0.99202757 0.12602102]\n",
      " [0.98916225 0.14682655]\n",
      " [0.98842426 0.15171516]\n",
      " [0.97688607 0.21376063]\n",
      " [0.9731012  0.23037807]\n",
      " [0.99009867 0.14037316]\n",
      " [0.99468088 0.10300458]\n",
      " [0.98810609 0.15377374]\n",
      " [0.98521175 0.17134117]\n",
      " [0.96699774 0.25478494]\n",
      " [0.98994949 0.14142136]\n",
      " [0.99313879 0.11694166]\n",
      " [0.98942047 0.14507631]\n",
      " [0.9923159  0.12373016]\n",
      " [0.98995892 0.14135539]\n",
      " [0.99283152 0.11952225]\n",
      " [0.97659027 0.21510799]\n",
      " [0.98150229 0.19145038]\n",
      " [0.99841143 0.05634376]\n",
      " [0.98816699 0.15338185]\n",
      " [0.9815078  0.19142217]\n",
      " [0.99434827 0.10616739]\n",
      " [0.98092896 0.1943666 ]\n",
      " [0.98582805 0.16775892]\n",
      " [0.97940711 0.20189532]\n",
      " [0.96308275 0.26920552]\n",
      " [0.97780241 0.20952909]\n",
      " [0.99062113 0.1366374 ]\n",
      " [0.99452547 0.1044944 ]\n",
      " [0.99437962 0.1058734 ]\n",
      " [0.98811258 0.15373202]\n",
      " [0.9801647  0.19818466]\n",
      " [0.98595419 0.16701596]\n",
      " [0.99022651 0.13946852]\n",
      " [0.98967697 0.14331605]\n",
      " [0.99272781 0.12038062]\n",
      " [0.98102995 0.19385622]\n",
      " [0.99225511 0.12421674]\n",
      " [0.98497764 0.17268191]\n",
      " [0.99222995 0.12441755]\n",
      " [0.98021649 0.19792833]\n",
      " [0.99021357 0.1395603 ]\n",
      " [0.98524568 0.17114596]\n",
      " [0.99064659 0.1364527 ]\n",
      " [0.9849053  0.17309408]\n",
      " [0.99546798 0.09509729]\n",
      " [0.99692399 0.07837453]\n",
      " [0.99039401 0.13827405]\n",
      " [0.99110348 0.13309357]\n",
      " [0.98695377 0.16100388]\n",
      " [0.98452739 0.17523078]\n",
      " [0.98938373 0.14532664]\n",
      " [0.98176841 0.19008101]\n",
      " [0.99015833 0.13995171]\n",
      " [0.99227788 0.12403473]\n",
      " [0.97885648 0.20454828]\n",
      " [0.98051586 0.19643993]\n",
      " [0.98145122 0.19171205]\n",
      " [0.99271283 0.12050411]\n",
      " [0.98334222 0.18176381]\n",
      " [0.9903434  0.13863602]\n",
      " [0.98424492 0.17681046]\n",
      " [0.9973489  0.07276795]\n",
      " [0.99070276 0.13604427]\n",
      " [0.98471252 0.17418739]\n",
      " [0.96632031 0.25734229]\n",
      " [0.98457858 0.17494289]\n",
      " [0.98859692 0.15058597]\n",
      " [0.98763272 0.15678526]\n",
      " [0.99669804 0.0811974 ]\n",
      " [0.99492193 0.10064966]\n",
      " [0.98549631 0.16969685]\n",
      " [0.99465602 0.10324435]\n",
      " [0.98893635 0.14834045]\n",
      " [0.98152754 0.1913209 ]\n",
      " [0.99272506 0.12040328]\n",
      " [0.98700727 0.1606756 ]\n",
      " [0.98105465 0.19373117]\n",
      " [0.99247442 0.12245212]\n",
      " [0.98967534 0.14332735]\n",
      " [0.98563832 0.16887007]\n",
      " [0.99007829 0.14051682]\n",
      " [0.99422916 0.10727713]\n",
      " [0.9900802  0.14050334]\n",
      " [0.98863996 0.15030315]\n",
      " [0.98275687 0.18490251]\n",
      " [0.98117767 0.1931072 ]\n",
      " [0.98060452 0.19599686]\n",
      " [0.81525026 0.57910881]\n",
      " [0.99505264 0.09934906]\n",
      " [0.97286824 0.23135987]\n",
      " [0.99556187 0.09410934]\n",
      " [0.99380899 0.11110218]\n",
      " [0.99059557 0.13682259]\n",
      " [0.92307692 0.38461538]\n",
      " [0.99478667 0.10197784]\n",
      " [0.99440752 0.10561096]\n",
      " [0.971905   0.23537348]\n",
      " [0.99703505 0.07694868]\n",
      " [0.96283011 0.27010771]\n",
      " [0.99727446 0.07378109]\n",
      " [0.97201936 0.23490076]\n",
      " [0.98626609 0.16516418]\n",
      " [0.99785133 0.0655188 ]\n",
      " [0.98190985 0.18934901]\n",
      " [0.98533672 0.17062108]\n",
      " [0.99546119 0.09516837]\n",
      " [0.97835547 0.20693132]\n",
      " [0.99597319 0.08965159]\n",
      " [0.99159327 0.12939395]\n",
      " [0.97210987 0.2345259 ]\n",
      " [0.98233857 0.18711211]\n",
      " [0.9818027  0.18990381]\n",
      " [0.99334115 0.11521006]\n",
      " [0.9883717  0.15205718]\n",
      " [0.98921918 0.14644251]\n",
      " [0.97790174 0.20906504]\n",
      " [0.98778182 0.15584313]\n",
      " [0.9883717  0.15205718]\n",
      " [0.98857982 0.15069814]\n",
      " [0.98154316 0.19124075]\n",
      " [0.98130713 0.19244821]\n",
      " [0.99187684 0.12720196]\n",
      " [0.99701346 0.077228  ]\n",
      " [0.99652023 0.0833513 ]\n",
      " [0.98506977 0.17215561]\n",
      " [0.99779396 0.06638682]\n",
      " [0.98071158 0.19546048]\n",
      " [0.98309941 0.18307252]\n",
      " [0.98271253 0.18513801]\n",
      " [0.97439703 0.22483424]\n",
      " [0.99638411 0.08496299]\n",
      " [0.99436913 0.10597184]\n",
      " [0.99172875 0.12835143]\n",
      " [0.98313005 0.18290792]\n",
      " [0.99478573 0.10198699]\n",
      " [0.98402491 0.17803082]\n",
      " [0.96798392 0.25101222]\n",
      " [0.97586485 0.21837535]\n",
      " [0.98283039 0.18451134]\n",
      " [0.9931405  0.11692712]\n",
      " [0.98839977 0.15187458]\n",
      " [0.98091575 0.19443325]\n",
      " [0.98941028 0.14514576]\n",
      " [0.97966223 0.20065371]\n",
      " [0.99163038 0.1291092 ]\n",
      " [0.99654862 0.08301113]\n",
      " [0.98220682 0.18780245]\n",
      " [0.95991662 0.28028573]\n",
      " [0.99081337 0.13523636]\n",
      " [0.98184827 0.18966805]\n",
      " [0.99238364 0.12318566]\n",
      " [0.99686539 0.0791163 ]\n",
      " [0.99936557 0.03561553]\n",
      " [0.99161275 0.12924453]\n",
      " [0.97983374 0.19981453]\n",
      " [0.99730368 0.07338511]\n",
      " [0.99613098 0.08788099]\n",
      " [0.99521256 0.09773412]\n",
      " [0.98747998 0.15774441]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "unit_vector = normalize(df[['total_bill', 'tip']])\n",
    "print(unit_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ab0bd",
   "metadata": {},
   "source": [
    "\n",
    "**Q3. Principal Component Analysis (PCA):**\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original features of a dataset into a new set of uncorrelated variables called principal components. These principal components are linear combinations of the original features and are ordered by the amount of variance they capture in the data. PCA is commonly used to reduce the dimensionality of high-dimensional datasets while retaining as much variance as possible.\n",
    "\n",
    "*How PCA Works:*\n",
    "\n",
    "- Calculate Covariance Matrix: Compute the covariance matrix of the original features.\n",
    "- Compute Eigenvectors and Eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "- Sort Eigenvectors: Sort the eigenvectors in descending order based on their corresponding eigenvalues.\n",
    "- Choose Principal Components: Select the top k eigenvectors to form the principal components, where k is the desired reduced dimensionality.\n",
    "- Transform Data: Multiply the original data by the selected eigenvectors to obtain the reduced-dimensional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5da338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 8]\n",
      " [7 7]\n",
      " [9 8]\n",
      " [6 9]\n",
      " [3 5]]\n",
      "[[-3.16208542 -1.3860793 ]\n",
      " [ 1.46025281  0.76659099]\n",
      " [ 3.63985804  0.26727042]\n",
      " [ 0.96093223 -1.41301424]\n",
      " [-2.89895766  1.76523214]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "import numpy as np\n",
    "data = np.array([[2, 8] , [7, 7] , [9, 8] , [6, 9], [3, 5]])\n",
    "print(data)\n",
    "pca = PCA(n_components=2)\n",
    "principalComp = pca.fit_transform(data)\n",
    "print(principalComp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3cb793",
   "metadata": {},
   "source": [
    "**Q4. Relationship between PCA and Feature Extraction:**\n",
    "\n",
    "Principal Component Analysis (PCA) is a feature extraction technique, and its primary goal is to transform the original features of a dataset into a set of uncorrelated variables known as principal components. Feature extraction involves selecting a subset of the most relevant features or transforming the original features into a new representation that retains essential information. PCA achieves this by identifying the directions (principal components) in which the data varies the most.\n",
    "\n",
    "*How PCA is Used for Feature Extraction:*\n",
    "\n",
    "- Capture Variance: PCA identifies the directions (principal components) along which the data has the highest variance.\n",
    "- Rank Principal Components: The principal components are ranked in descending order based on the amount of variance they capture.\n",
    "- Select Top Components: By selecting the top k principal components, where k is the desired reduced dimensionality, PCA effectively extracts a subset of features that retain the most significant information in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a8fe14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 8]\n",
      " [7 7]\n",
      " [9 8]\n",
      " [6 9]\n",
      " [3 5]]\n",
      "Extracted features:\n",
      " [[-3.16208542 -1.3860793 ]\n",
      " [ 1.46025281  0.76659099]\n",
      " [ 3.63985804  0.26727042]\n",
      " [ 0.96093223 -1.41301424]\n",
      " [-2.89895766  1.76523214]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "import numpy as np\n",
    "data = np.array([[2, 8] , [7, 7] , [9, 8] , [6, 9], [3, 5]])\n",
    "print(data)\n",
    "pca = PCA(n_components=2)\n",
    "principalComp = pca.fit_transform(data)\n",
    "print('Extracted features:\\n',principalComp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b7264",
   "metadata": {},
   "source": [
    "**Q5**\n",
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be a valuable preprocessing step to ensure that the features, such as price, rating, and delivery time, are on a consistent scale. This is important because recommendation algorithms often rely on distance or similarity measures between items or users, and having features on a similar scale helps prevent certain features from dominating the recommendation process. Here's how you could use Min-Max scaling for this purpose:\n",
    "\n",
    "1. **Understand the Data:**\n",
    "   - Examine the distribution and range of values for each feature (price, rating, delivery time) in the dataset.\n",
    "\n",
    "2. **Choose Relevant Features:**\n",
    "   - Identify which features you want to include in your recommendation model. In this case, it could be price, rating, and delivery time.\n",
    "\n",
    "3. **Apply Min-Max Scaling:**\n",
    "   - For each selected feature, apply Min-Max scaling to transform the values to a specific range, commonly [0, 1]..\n",
    "\n",
    "4. **Normalization Process:**\n",
    "   - Normalize each feature independently, ensuring that all of them are transformed to the [0, 1] range.\n",
    "   - This process preserves the relative relationships between values while ensuring that each feature contributes equally to the recommendation process.\n",
    "\n",
    "5. **Implementation in Python:**\n",
    "   - Here's a simplified example using Python and scikit-learn:\n",
    "    o_scale] = normalized_features\n",
    "\n",
    "6. **Evaluate and Monitor:**\n",
    "   - Assess the impact of Min-Max scaling on your recommendation system's performance.\n",
    "   - Monitor the recommendation system's behavior and adjust scaling parameters if needed.\n",
    "\n",
    "By using Min-Max scaling, you ensure that the different features contribute more uniformly to the recommendation process, helping to improve the fairness and effectiveness of your recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b301057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04359673 0.06475904 0.04026846]\n",
      " [0.02152589 0.         0.02684564]\n",
      " [0.0373297  0.07108434 0.04362416]\n",
      " ...\n",
      " [0.11280654 0.         0.10067114]\n",
      " [0.03051771 0.         0.03355705]\n",
      " [0.10490463 0.10120482 0.09395973]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('taxis')\n",
    "m_M_scaler = MinMaxScaler()\n",
    "normalized = m_M_scaler.fit_transform(df[['distance', 'tip', 'fare']])\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec8036b",
   "metadata": {},
   "source": [
    "**Q6**\n",
    "Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset for predicting stock prices involves transforming the original features into a smaller set of uncorrelated variables, called principal components. This process helps capture the most significant variability in the data while reducing the number of features, which can be especially beneficial when dealing with datasets containing numerous features. Here's a step-by-step explanation of how you might use PCA for this purpose:\n",
    "\n",
    "1. **Understand the Dataset:**\n",
    "   - Review the dataset to identify the features related to company financial data and market trends. This could include variables like revenue, earnings, market indices, etc.\n",
    "\n",
    "2. **Standardize the Data:**\n",
    "   - It's often a good practice to standardize the data (subtract the mean and divide by the standard deviation) before applying PCA. Standardization ensures that all features have a mean of 0 and a standard deviation of 1, preventing features with larger scales from dominating the principal components.\n",
    "\n",
    "3. **Apply PCA:**\n",
    "   - Use PCA to transform the standardized features into principal components. Specify the number of components you want to retain based on the desired dimensionality reduction.\n",
    "   - The scikit-learn library provides a convenient implementation of PCA. Here's an example:\n",
    "     ```python\n",
    "     from sklearn.decomposition import PCA\n",
    "     from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "     # Assuming 'data' is your DataFrame with financial and market trend features\n",
    "     features_to_scale = ['feature1', 'feature2', 'feature3', ...]  # List the features you want to include\n",
    "\n",
    "     # Standardize the data\n",
    "     scaler = StandardScaler()\n",
    "     standardized_data = scaler.fit_transform(data[features_to_scale])\n",
    "\n",
    "     # Apply PCA\n",
    "     n_components = 5  # Choose the number of components based on your analysis\n",
    "     pca = PCA(n_components=n_components)\n",
    "     reduced_data = pca.fit_transform(standardized_data)\n",
    "     ```\n",
    "\n",
    "4. **Evaluate Explained Variance:**\n",
    "   - Examine the explained variance ratio to understand how much variance is retained by each principal component. This information helps you decide on the appropriate number of components to retain.\n",
    "     ```python\n",
    "     explained_variance_ratio = pca.explained_variance_ratio_\n",
    "     print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "     ```\n",
    "\n",
    "5. **Interpret Principal Components:**\n",
    "   - Analyze the loadings of the principal components to interpret the relationships between the original features and the components. This can provide insights into which features contribute most to the reduced-dimensional representation.\n",
    "\n",
    "6. **Model Building:**\n",
    "   - Use the reduced-dimensional data (principal components) for training your stock price prediction model. The reduced set of features can help improve model efficiency and mitigate the curse of dimensionality.\n",
    "\n",
    "7. **Monitoring and Fine-Tuning:**\n",
    "   - Monitor the performance of your stock price prediction model and consider fine-tuning the number of components based on performance metrics and domain knowledge.\n",
    "\n",
    "By applying PCA, you aim to retain the essential information in the dataset while reducing the number of features, which can be beneficial for building efficient and effective stock price prediction models./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab54de28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   data\n",
      "0     1\n",
      "1     5\n",
      "2    10\n",
      "3    15\n",
      "4    20\n",
      "[[0.        ]\n",
      " [0.21052632]\n",
      " [0.47368421]\n",
      " [0.73684211]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Q7\n",
    "data = [1, 5, 10, 15, 20]\n",
    "df1 = pd.DataFrame(data, columns = [\"data\"])\n",
    "print(df1)\n",
    "minmax = m_M_scaler.fit_transform(df1[['data']])\n",
    "print(minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc197c4",
   "metadata": {},
   "source": [
    "The decision of how many principal components to retain in a feature extraction process using PCA depends on the desired trade-off between dimensionality reduction and the amount of variance retained. The goal is to choose a sufficient number of principal components that capture most of the variability in the original dataset while reducing the dimensionality.\n",
    "\n",
    "Here are the general steps you might follow:\n",
    "\n",
    "1. **Standardize the Data:**\n",
    "   - Ensure that the data is standardized, with each feature having a mean of 0 and a standard deviation of 1. This step is crucial for PCA to work effectively, especially when features are on different scales.\n",
    "\n",
    "2. **Apply PCA:**\n",
    "   - Apply PCA to the standardized data and examine the explained variance ratio to understand how much variance each principal component captures.\n",
    "     ```python\n",
    "     from sklearn.decomposition import PCA\n",
    "     from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "     # Assuming 'data' is your DataFrame with features\n",
    "     features_to_scale = ['height', 'weight', 'age', 'blood_pressure']\n",
    "\n",
    "     # Standardize the data\n",
    "     scaler = StandardScaler()\n",
    "     standardized_data = scaler.fit_transform(data[features_to_scale])\n",
    "\n",
    "     # Apply PCA\n",
    "     pca = PCA()\n",
    "     pca.fit(standardized_data)\n",
    "\n",
    "     # Examine the explained variance ratio\n",
    "     explained_variance_ratio = pca.explained_variance_ratio_\n",
    "     print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "     ```\n",
    "\n",
    "3. **Determine the Number of Principal Components:**\n",
    "   - Analyze the cumulative explained variance to decide how many principal components to retain. You may choose a threshold, such as retaining components that explain, for example, 95% of the variance.\n",
    "     ```python\n",
    "     cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "     ```\n",
    "\n",
    "4. **Visualize the Cumulative Explained Variance:**\n",
    "   - Plot the cumulative explained variance to visualize the trade-off between the number of principal components and the total variance captured.\n",
    "     ```python\n",
    "     import matplotlib.pyplot as plt\n",
    "\n",
    "     plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o')\n",
    "     plt.xlabel('Number of Principal Components')\n",
    "     plt.ylabel('Cumulative Explained Variance')\n",
    "     plt.title('Cumulative Explained Variance vs. Number of Principal Components')\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "5. **Select the Number of Components:**\n",
    "   - Choose the number of principal components that explain a sufficiently high percentage of the variance. This is typically done by inspecting the plot or looking at the explained variance ratio.\n",
    "\n",
    "6. **Final PCA Transformation:**\n",
    "   - Transform the original data using the selected number of principal components.\n",
    "     ```python\n",
    "     n_components = 3  # Choose an appropriate number based on your analysis\n",
    "     pca = PCA(n_components=n_components)\n",
    "     reduced_data = pca.fit_transform(standardized_data)\n",
    "     ```\n",
    "\n",
    "In the context of your dataset with features [height, weight, age, gender, blood pressure], you might choose the number of principal components based on the cumulative explained variance. The goal is to retain enough components to capture a high percentage of the total variance while achieving dimensionality reduction. The specific choice may depend on the application's requirements and the desired balance between reduced dimensionality and retained information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
